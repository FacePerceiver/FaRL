# args: [lr_factor]

optimizer_type: torch.optim.AdamW
optimizer_args:
  lr: $0.001 * ARGS.get('lr_factor', 1.0)
  betas: [0.9, 0.999]
  weight_decay: 0.00001
lr_scheduler_type: torch.optim.lr_scheduler.MultiStepLR
lr_scheduler_args:
  milestones: [200]
  gamma: 0.1
lr_scheduler_call: epochwise
network_settings:
  - params: main.backbone.visual
    lr: $0.0001 * ARGS.get('lr_factor', 1.0)
  - params: main.backbone.fpns
    lr: $0.01 * ARGS.get('lr_factor', 1.0)
  - params: main.heatmap_head
    lr: $0.01 * ARGS.get('lr_factor', 1.0)